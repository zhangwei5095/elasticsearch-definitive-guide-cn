## 分析和分析器

**分析(analysis)**是这样一个过程：

* 首先，标记化一个文本块为适用于倒排索引单独的**词(term)**
* 然后标准化这些词为标准形式，提高它们的“可搜索性”或“查全率”

这个工作是**分析器(analyzer)**完成的。一个**分析器(analyzer)**只是一个包装用于将三个功能放到一个包里：

### 字符过滤器

首先字符串经过**字符过滤器(character filter)**，它们的工作是在标记化前处理字符串。字符过滤器能够去除HTML标记，或者转换`"&"`为`"and"`。

### 分词器

下一步，**分词器(tokenizer)**被标记化成独立的词。一个简单的**分词器(tokenizer)**可以根据空格或逗号将单词分开（译者注：这个在中文中不适用）。

### 标记过滤

最后，每个词都通过所有**标记过滤(token filters)**，它可以修改词（例如将`"Quick"`转为小写），去掉词（例如停用词像`"a"`、`"and"`、`"the"`等等），或者增加词（例如同义词像`"jump"`和`"leap"`）

Elasticsearch提供很多开箱即用的字符过滤器，分词器和标记过滤器。这些可以组合来创建自定义的分析器以应对不同的需求。我们将在《自定义分析器》章节详细讨论。

### 内建的分析器

不过，Elasticsearch还附带了一些预装的分析器，你可以直接使用它们。下面我们列出了最重要的几个分析器，来演示这个字符串分词后的表现差异：

    "Set the shape to semi-transparent by calling set_trans(5)"


### 标准分析器

标准分析器是Elasticsearch默认使用的分析器。对于文本分析，它对于任何语言都是最佳选择（译者注：就是没啥特殊需求，对于任何一个国家的语言，这个分析器就够用了）。它根据[Unicode Consortium](http://www.unicode.org/reports/tr29/)的定义的**单词边界(word boundaries)**来切分文本，然后去掉大部分标点符号。最后，把所有词转为小写。产生的结果为：

    set, the, shape, to, semi, transparent, by, calling, set_trans, 5

### 简单分析器

简单分析器将非单个字母的文本切分，然后把每个词转为小写。产生的结果为：

    set, the, shape, to, semi, transparent, by, calling, set, trans

### 空格分析器

空格分析器依据空格切分文本。它不转换小写。产生结果为：

    Set, the, shape, to, semi-transparent, by, calling, set_trans(5)

### 语言分析器

特定语言分析器适用于很多语言。它们能够考虑到特定语言的特性。例如，`english`分析器自带一套英语停用词库——像`and`或`the`这些与语义无关的通用词。这些词被移除后，因为语法规则的存在，英语单词的主体含义依旧能被理解（译者注：`stem English words`这句不知道该如何翻译，查了字典，我理解的大概意思应该是将英语语句比作一株植物，去掉无用的枝叶，主干依旧存在，停用词好比枝叶，存在与否并不影响对这句话的理解。）。

`english`分析器将会产生以下结果：

    set, shape, semi, transpar, call, set_tran, 5

注意`"transparent"`、`"calling"`和`"set_trans"`是如何转为词干的。

### 当分析器被使用

当我们**索引(index)**一个文档，全文字段会被分析为单独的词来创建倒排索引。不过，当我们在全文字段**搜索(search)**时，我们要让查询字符串经过**同样的分析流程**处理，以确保这些词在索引中存在。

全文查询我们将在稍后讨论，理解每个字段是如何定义的，这样才可以让它们做正确的事：

* 当你查询**全文(full text)**字段，查询将使用相同的分析器来分析查询字符串，以产生正确的词列表。
* 当你查询一个**确切值(exact value)**字段，查询将不分析查询字符串，但是你可以自己指定。

现在你可以明白为什么《映射和分析》的开头会产生那种结果：
* `date`字段包含一个确切值：单独的一个词`"2014-09-15"`。
* `_all`字段是一个全文字段，所以分析过程将日期转为三个词：`"2014"`、`"09"`和`"15"`。

当我们在`_all`字段查询`2014`，它一个匹配到12条推文，因为这些推文都包含词`2014`：

```sh
GET /_search?q=2014              # 12 results
```

当我们在`_all`字段中查询`2014-09-15`，首先分析查询字符串，产生匹配**任一**词`2014`、`09`或`15`的查询语句，它依旧匹配12个推文，因为它们都包含词`2014`。

```sh
GET /_search?q=2014-09-15        # 12 results !
```

当我们在`date`字段中查询`2014-09-15`，它查询一个**确切**的日期，然后只找到一条推文：

```sh
GET /_search?q=date:2014-09-15   # 1  result
```

当我们在`date`字段中查询`2014`，没有找到文档，因为没有文档包含那个确切的日期：

```sh
GET /_search?q=date:2014         # 0  results !
```

### 测试分析器

尤其当你是Elasticsearch新手时，对于如何分词以及存储到索引中理解起来比较困难。为了更好的理解如何进行，你可以使用`analyze` API来查看文本是如何被分析的。在查询字符串参数中指定要使用的分析器，被分析的文本做为请求体：

```javascript
GET /_analyze?analyzer=standard&text=Text to analyze
```

结果中每个节点在代表一个词：

```Javascript
{
   "tokens": [
      {
         "token":        "text",
         "start_offset": 0,
         "end_offset":   4,
         "type":         "<ALPHANUM>",
         "position":     1
      },
      {
         "token":        "to",
         "start_offset": 5,
         "end_offset":   7,
         "type":         "<ALPHANUM>",
         "position":     2
      },
      {
         "token":        "analyze",
         "start_offset": 8,
         "end_offset":   15,
         "type":         "<ALPHANUM>",
         "position":     3
      }
   ]
}
```

`token`是一个实际被存储在索引中的词。`position`指明词在原文本中是第几个出现的。`start_offset`和`end_offset`表示词在原文本中占据的位置。

`analyze` API 对于理解Elasticsearch索引的内在细节是个非常有用的工具，随着内容的推进，我们将继续讨论它。

### 指定分析器

当Elasticsearch在你的文档中探测到一个新的字符串字段，它将自动设置它为全文`string`字段并用`standard`分析器分析。

你不可能总是想要这样做。也许你想使用一个更适合这个数据的语言分析器。或者，你只想把字符串字段当作一个普通的字段——不做任何分析，只存储确切值，就像字符串类型的用户ID或者内部状态字段或者标签。

为了达到这种效果，我们必须通过**映射(mapping)**人工设置这些字段。
